{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Dissertation/experiments/idiom_principle_on_magpie_corpus/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-5mVwtBkQ_9",
        "outputId": "2f6cbc1b-3af1-4f68-832d-3281aa40495d"
      },
      "id": "k-5mVwtBkQ_9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Dissertation/experiments/idiom_principle_on_magpie_corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r ./requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMirIspRwePf",
        "outputId": "3236075f-9afe-4ecc-ee1c-1af13a2346e8"
      },
      "id": "FMirIspRwePf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.7.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 1)) (4.7.0)\n",
            "Requirement already satisfied: datasets==1.6.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: tqdm==4.49.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 3)) (4.49.0)\n",
            "Requirement already satisfied: nltk==3.6.2 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 4)) (3.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (0.0.53)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (4.12.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (0.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0->-r ./requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.1->-r ./requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.1->-r ./requirements.txt (line 2)) (0.70.13)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.1->-r ./requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.1->-r ./requirements.txt (line 2)) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.1->-r ./requirements.txt (line 2)) (0.3.5.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.1->-r ./requirements.txt (line 2)) (2022.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2->-r ./requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2->-r ./requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0->-r ./requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0->-r ./requirements.txt (line 1)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0->-r ./requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0->-r ./requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0->-r ./requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0->-r ./requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0->-r ./requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.1->-r ./requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.1->-r ./requirements.txt (line 2)) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.6.1->-r ./requirements.txt (line 2)) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment: Exp0\n",
        "TODO"
      ],
      "metadata": {
        "id": "KCZSTnEIwzeW"
      },
      "id": "KCZSTnEIwzeW"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth=500\n",
        "\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "GAttiuDPwnCc"
      },
      "id": "GAttiuDPwnCc",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from exp_helpers import run_glue_f1_macro"
      ],
      "metadata": {
        "id": "fENgdXWYx6cA"
      },
      "id": "fENgdXWYx6cA",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Setup"
      ],
      "metadata": {
        "id": "Uo3wGcP75Zaa"
      },
      "id": "Uo3wGcP75Zaa"
    },
    {
      "cell_type": "code",
      "source": [
        "exp_name = 'exp0'\n",
        "exp_model = 'bert-base-cased'\n",
        "exp_seed = 26"
      ],
      "metadata": {
        "id": "E3ff8t4P3Qop"
      },
      "id": "E3ff8t4P3Qop",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5f1f49ed",
      "metadata": {
        "id": "5f1f49ed"
      },
      "outputs": [],
      "source": [
        "base_dir = 'data/magpie/'\n",
        "# NOTE: This notebook should ideally modify only the contents of this exp_dir.\n",
        "exp_dir = 'experiments/' + exp_name + '/'\n",
        "data_file = base_dir + 'processed_MAGPIE_filtered_split_typebased.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(exp_dir):\n",
        "    print(f'ERROR! The experiment directory {exp_dir} already exists!')\n",
        "    print(f\"Run '%rm -rf {exp_dir}\")\n",
        "    assert not os.path.isdir(exp_dir)\n",
        "\n",
        "tmp_dir = exp_dir + 'tmp/'\n",
        "model_checkpoint_dir = exp_dir + 'models/'\n",
        "\n",
        "if not os.path.isdir(tmp_dir):\n",
        "    os.makedirs(tmp_dir)\n",
        "\n",
        "if not os.path.isdir(model_checkpoint_dir):\n",
        "    os.makedirs(model_checkpoint_dir)"
      ],
      "metadata": {
        "id": "Mh4PU__T3J4K"
      },
      "id": "Mh4PU__T3J4K",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fe3ba613",
      "metadata": {
        "scrolled": true,
        "id": "fe3ba613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "131a2e18-5afe-4ffb-9f0e-84863edcf2a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                 sentence_0  \\\n",
              "0                               For example , with fell running and mountain marathons gaining in popularity , how about some ideas for safe running off the beaten track ?   \n",
              "1                                                                                                                                       I 'd keep him well in the running .   \n",
              "2                                                                                                                    He gives me the creeps , so I looked round , hmm hmm .   \n",
              "3                                                                                                                        ‘ He 's done us proud , as well,’ says Granville .   \n",
              "4                       People quickly embraced formal democracy , but the tolerance and compromise that is at the heart of the democratic process took time to take root .   \n",
              "...                                                                                                                                                                     ...   \n",
              "48390                                                                                                               Many also have second or third jobs to make ends meet .   \n",
              "48391                                           Take people to objections , take them to where you want them to be and bear in mind you 're always looking for an objection   \n",
              "48392                Indeed we are rarely aware of them as rules , until they are broken , since they are typical of the settings in which we received our moral training .   \n",
              "48393  Unlike in a firm that is a jack of all trades , the supplier is an independent business subject to market disciplines rather than another bit of a big bureaucracy .   \n",
              "48394                                                                The Government flies these kites of disinformation then people feel grateful when they do n't happen .   \n",
              "\n",
              "                         idiom  confidence label     split  \\\n",
              "0         off the beaten track    1.000000     i  training   \n",
              "1               in the running    0.770109     i  training   \n",
              "2      give someone the creeps    1.000000     i  training   \n",
              "3             do someone proud    1.000000     i  training   \n",
              "4                    take root    1.000000     i  training   \n",
              "...                        ...         ...   ...       ...   \n",
              "48390           make ends meet    0.854973     i      test   \n",
              "48391             bear in mind    1.000000     i  training   \n",
              "48392                as a rule    1.000000     l  training   \n",
              "48393       jack of all trades    1.000000     i  training   \n",
              "48394               fly a kite    1.000000     i  training   \n",
              "\n",
              "              variant_type  \n",
              "0                identical  \n",
              "1                identical  \n",
              "2      combined-inflection  \n",
              "3      combined-inflection  \n",
              "4                identical  \n",
              "...                    ...  \n",
              "48390            identical  \n",
              "48391            identical  \n",
              "48392  deletion-determiner  \n",
              "48393            identical  \n",
              "48394  combined-inflection  \n",
              "\n",
              "[48395 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6255f6a-444b-48ea-a669-1fef65c9786d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_0</th>\n",
              "      <th>idiom</th>\n",
              "      <th>confidence</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "      <th>variant_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>For example , with fell running and mountain marathons gaining in popularity , how about some ideas for safe running off the beaten track ?</td>\n",
              "      <td>off the beaten track</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>identical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I 'd keep him well in the running .</td>\n",
              "      <td>in the running</td>\n",
              "      <td>0.770109</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>identical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>He gives me the creeps , so I looked round , hmm hmm .</td>\n",
              "      <td>give someone the creeps</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>combined-inflection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>‘ He 's done us proud , as well,’ says Granville .</td>\n",
              "      <td>do someone proud</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>combined-inflection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>People quickly embraced formal democracy , but the tolerance and compromise that is at the heart of the democratic process took time to take root .</td>\n",
              "      <td>take root</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>identical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48390</th>\n",
              "      <td>Many also have second or third jobs to make ends meet .</td>\n",
              "      <td>make ends meet</td>\n",
              "      <td>0.854973</td>\n",
              "      <td>i</td>\n",
              "      <td>test</td>\n",
              "      <td>identical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48391</th>\n",
              "      <td>Take people to objections , take them to where you want them to be and bear in mind you 're always looking for an objection</td>\n",
              "      <td>bear in mind</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>identical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48392</th>\n",
              "      <td>Indeed we are rarely aware of them as rules , until they are broken , since they are typical of the settings in which we received our moral training .</td>\n",
              "      <td>as a rule</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>l</td>\n",
              "      <td>training</td>\n",
              "      <td>deletion-determiner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48393</th>\n",
              "      <td>Unlike in a firm that is a jack of all trades , the supplier is an independent business subject to market disciplines rather than another bit of a big bureaucracy .</td>\n",
              "      <td>jack of all trades</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>identical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48394</th>\n",
              "      <td>The Government flies these kites of disinformation then people feel grateful when they do n't happen .</td>\n",
              "      <td>fly a kite</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>i</td>\n",
              "      <td>training</td>\n",
              "      <td>combined-inflection</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48395 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6255f6a-444b-48ea-a669-1fef65c9786d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6255f6a-444b-48ea-a669-1fef65c9786d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6255f6a-444b-48ea-a669-1fef65c9786d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_data = pd.read_csv(data_file)\n",
        "df_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6f7355ab",
      "metadata": {
        "id": "6f7355ab"
      },
      "outputs": [],
      "source": [
        "columns=['sentence_0', 'idiom', 'confidence', 'label', 'split', 'variant_type']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare & save the train, dev & test sets"
      ],
      "metadata": {
        "id": "C52nDzJG09z6"
      },
      "id": "C52nDzJG09z6"
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id = {'i': 0, 'l': 1}"
      ],
      "metadata": {
        "id": "F8XjnVNz2PuK"
      },
      "id": "F8XjnVNz2PuK",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data['split'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBTb2rUkwJK8",
        "outputId": "69dafe0a-b528-4a64-9d7d-c38b9b76ca98"
      },
      "id": "DBTb2rUkwJK8",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "training       38715\n",
              "test            4840\n",
              "development     4840\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp = df_data[['sentence_0', 'label', 'split']]\n",
        "\n",
        "df_train = df_tmp[df_tmp['split'] == 'training']\n",
        "df_dev = df_tmp[df_tmp['split'] == 'development']\n",
        "df_test = df_tmp[df_tmp['split'] == 'test']\n",
        "\n",
        "def clean_df(df):\n",
        "    \"\"\"Clean each of the datasets\"\"\"\n",
        "    df = df.drop(columns=['split'])\n",
        "    df['label'] = df['label'].map(label_to_id)\n",
        "    return df\n",
        "\n",
        "# Clean the datasets\n",
        "df_train, df_dev, df_test = [clean_df(df) for df in [df_train, df_dev, df_test]]"
      ],
      "metadata": {
        "id": "OqaBa6Pl004w"
      },
      "id": "OqaBa6Pl004w",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data to tmp files\n",
        "train_csv = tmp_dir + 'train.csv'\n",
        "dev_csv = tmp_dir + 'dev.csv'\n",
        "test_csv = tmp_dir + 'test.csv'\n",
        "\n",
        "df_train.to_csv(train_csv, index=False)\n",
        "df_dev.to_csv(dev_csv, index=False)\n",
        "df_test.to_csv(test_csv, index=False)\n",
        "print(f'Saved the files to {tmp_dir}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoN6C7nj1z8E",
        "outputId": "b617a935-c28b-4d81-f52f-cdfe249aef8a"
      },
      "id": "AoN6C7nj1z8E",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved the files to experiments/exp0/tmp/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Evaluation"
      ],
      "metadata": {
        "id": "1oSMKsaa4jU0"
      },
      "id": "1oSMKsaa4jU0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Fine-tune & Save the binary classification model\n",
        "\n",
        "TODO: Note that `dev_csv` file is not used anywhere here!"
      ],
      "metadata": {
        "id": "PWUrf9a44nfL"
      },
      "id": "PWUrf9a44nfL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the helper script that trains, saves the sentence classification model\n",
        "!python exp_helpers/run_glue_f1_macro.py \\\n",
        "        --model_name_or_path $exp_model \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir $model_checkpoint_dir \\\n",
        "    \t--seed $exp_seed \\\n",
        "    \t--train_file      $train_csv \\\n",
        "    \t--validation_file $test_csv \\\n",
        "        --evaluation_strategy \"epoch\" \\\n",
        "        --save_strategy \"epoch\"  \\\n",
        "        --load_best_model_at_end \\\n",
        "        --metric_for_best_model \"f1\" \\\n",
        "        --save_total_limit 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY7soeu042NQ",
        "outputId": "b233d0af-9ce5-47a3-974b-f5f9ac343f20"
      },
      "id": "pY7soeu042NQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/13/2022 23:43:31 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "07/13/2022 23:43:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=runs/Jul13_23-43-31_9baa6f96f100,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "output_dir=experiments/exp0/models/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=experiments/exp0/models/,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=3,\n",
            "seed=26,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/13/2022 23:43:31 - INFO - __main__ -   load a local file for train: experiments/exp0/tmp/train.csv\n",
            "07/13/2022 23:43:31 - INFO - __main__ -   load a local file for validation: experiments/exp0/tmp/test.csv\n",
            "07/13/2022 23:43:32 - WARNING - datasets.builder -   Using custom data configuration default-7febc6c0fb2a16f8\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-7febc6c0fb2a16f8/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "0 tables [00:00, ? tables/s]/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:1133: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  for obj in iterable:\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7febc6c0fb2a16f8/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1590] 2022-07-13 23:43:32,928 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpa9hx53iy\n",
            "Downloading: 100% 570/570 [00:00<00:00, 301kB/s]\n",
            "[INFO|file_utils.py:1594] 2022-07-13 23:43:33,071 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|file_utils.py:1602] 2022-07-13 23:43:33,072 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:517] 2022-07-13 23:43:33,072 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:553] 2022-07-13 23:43:33,073 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:517] 2022-07-13 23:43:33,178 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:553] 2022-07-13 23:43:33,179 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1590] 2022-07-13 23:43:33,307 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpc93moa6e\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 2.87MB/s]\n",
            "[INFO|file_utils.py:1594] 2022-07-13 23:43:33,517 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1602] 2022-07-13 23:43:33,517 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1590] 2022-07-13 23:43:33,629 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp97fuhgyi\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 4.36MB/s]\n",
            "[INFO|file_utils.py:1594] 2022-07-13 23:43:33,861 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1602] 2022-07-13 23:43:33,861 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1590] 2022-07-13 23:43:34,171 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp01we4k92\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 20.9kB/s]\n",
            "[INFO|file_utils.py:1594] 2022-07-13 23:43:34,274 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1602] 2022-07-13 23:43:34,274 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|tokenization_utils_base.py:1717] 2022-07-13 23:43:34,274 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1717] 2022-07-13 23:43:34,275 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1717] 2022-07-13 23:43:34,275 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2022-07-13 23:43:34,275 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2022-07-13 23:43:34,275 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1590] 2022-07-13 23:43:34,398 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1saaosns\n",
            "Downloading: 100% 436M/436M [00:07<00:00, 58.7MB/s]\n",
            "[INFO|file_utils.py:1594] 2022-07-13 23:43:41,958 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[INFO|file_utils.py:1602] 2022-07-13 23:43:41,959 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[INFO|modeling_utils.py:1152] 2022-07-13 23:43:41,959 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[WARNING|modeling_utils.py:1328] 2022-07-13 23:43:43,805 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1339] 2022-07-13 23:43:43,805 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "07/13/2022 23:43:43 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f8f0be80320> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 39/39 [00:05<00:00,  6.63ba/s]\n",
            "100% 5/5 [00:00<00:00,  6.45ba/s]\n",
            "07/13/2022 23:43:50 - INFO - __main__ -   Sample 13445 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 4914, 117, 1114, 170, 187, 6094, 2165, 2653, 4386, 2158, 1137, 117, 4146, 1253, 117, 170, 187, 6094, 2165, 12966, 117, 1297, 1110, 12186, 1107, 11708, 6533, 4999, 1205, 1103, 11182, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence_0': 'Without , with a rouble paycheck or , worse still , a rouble pension , life is slipping inexorably down the tubes .', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/13/2022 23:43:50 - INFO - __main__ -   Sample 28341 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 786, 2048, 1139, 106, 787, 1163, 3162, 117, 1702, 1120, 1123, 1167, 5536, 1190, 1117, 1490, 1156, 1138, 3228, 117, 786, 1128, 1138, 2014, 1240, 9253, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence_0': '‘ Oh my!’ said Bob , looking at her more seriously than his voice would have suggested , ‘ you have changed your tune .', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/13/2022 23:43:50 - INFO - __main__ -   Sample 35652 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 13094, 1916, 1965, 1579, 3471, 1120, 1103, 1334, 1104, 1103, 23458, 113, 1170, 1103, 14516, 25043, 7562, 1138, 1151, 6561, 114, 1118, 16366, 170, 13094, 1113, 1296, 3111, 1104, 1594, 1643, 15060, 1107, 170, 10012, 2447, 1506, 1103, 9346, 1104, 1103, 23458, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence_0': 'The knotting process always begins at the side of the rug ( after the selvedges have been secured ) by tying a knot on each pair of warp strands in a horizontal direction across the width of the rug .', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:515] 2022-07-13 23:43:50,534 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence_0.\n",
            "[INFO|trainer.py:1147] 2022-07-13 23:43:50,606 >> ***** Running training *****\n",
            "[INFO|trainer.py:1148] 2022-07-13 23:43:50,606 >>   Num examples = 38715\n",
            "[INFO|trainer.py:1149] 2022-07-13 23:43:50,606 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1150] 2022-07-13 23:43:50,606 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1151] 2022-07-13 23:43:50,606 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1152] 2022-07-13 23:43:50,606 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1153] 2022-07-13 23:43:50,606 >>   Total optimization steps = 10890\n",
            "  0% 3/10890 [02:01<122:32:44, 40.52s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Evaluate the binary classification model\n",
        "\n",
        "TODO"
      ],
      "metadata": {
        "id": "WEoPG-5V4v-M"
      },
      "id": "WEoPG-5V4v-M"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rhuuyD-R43wt"
      },
      "id": "rhuuyD-R43wt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oP9-GAn7123z"
      },
      "id": "oP9-GAn7123z",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LAB_VENV",
      "language": "python",
      "name": "lab_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "exp0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}