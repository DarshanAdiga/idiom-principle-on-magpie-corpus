{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86614537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/darshan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad63ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:../../local_models/bert-base-uncased_MaskedLM_STR_option1_3B1_pretrained\n"
     ]
    }
   ],
   "source": [
    "local_model_base_dir = '../../local_models/'\n",
    "# Location of the model with single-tokens\n",
    "model_name = 'bert-base-uncased_MaskedLM_STR_option1_3B1_pretrained'\n",
    "model_checkpoint_dir = local_model_base_dir + model_name\n",
    "print(f'Model path:{model_checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50f3208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PIE_list': ['IDmybadID', 'IDgameonID', 'IDchaseyourtailID', 'IDstealtheshowID', 'IDfromscratchID', 'IDlendahandID', 'IDatseaID', 'IDbiteyourlipID', 'IDdaylightrobberyID', 'IDactofgodID', 'IDbreaktheiceID', 'IDrunamileID', 'IDunderthetableID', 'IDholdyourtongueID', 'IDputthekiboshonID', 'IDindutchID', 'IDinthedriversseatID'], 'paraphrases': ['contest', 'ready for something', 'rush around ineffectually', 'very busy', 'center of attention', 'outshine', 'from very beginning', 'assist', 'help', 'confused', 'puzzled', 'repress an emotion', 'unfair trade', 'victim', 'severe natural event', 'relieve tension', 'start a conversation', 'reluctant', 'extremely unwilling', 'secretly or covertly', 'very drunk', 'remain silent', 'put an end to', 'check', 'curb', 'stop', 'in trouble', 'in disfavor', 'be in control', \"in the driver's seat\", 'make the decisions'], 'words': []}\n"
     ]
    }
   ],
   "source": [
    "wordlist_set_NAME = 'set1'\n",
    "wordlist_set_json_file = '../../data/emb_visuals/wordlist_set1.json'\n",
    "\n",
    "# The common set of tokens, paraphrases and words to be considered in the final output\n",
    "with open(wordlist_set_json_file, 'r') as jfile:\n",
    "    WORDLIST_DICT = json.load(jfile)\n",
    "print(WORDLIST_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049b9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment name\n",
    "exp_name = 'exp3B_1'\n",
    "\n",
    "IS_BERTRAM_FORMAT = False\n",
    "\n",
    "token_PIE_mapping_file = '../../data/token_files/option1_idioms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c970d93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should BERTRAM format be used: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Should BERTRAM format be used: {IS_BERTRAM_FORMAT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906d7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "dump_dir = './embedding_dump/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacadfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "128ad95e",
   "metadata": {},
   "source": [
    "# Add paraphrases of sample PIEs\n",
    "In addition to the PIE single tokens and their constituent words, synonyms and paraphrases for some of the interesting PIEs are also included in the embedding space for the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520e0ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# paraphrases_and_synonyms = set()\n",
    "\n",
    "# nctti_synonyms = {\n",
    "#  'absolute lowest', 'all time low', 'basement', 'bliss', 'bottom floor', 'cheapest', 'cliffhanger', 'close one',\n",
    "#  'close shave', 'crazy', 'dated', 'defenseless', 'down low', 'easy prey', 'easy target', 'euphoria', 'ex girlfriend', 'exposed',\n",
    "#  'first floor', 'first story', 'food chain', 'former lover', 'ground level', 'heaven', 'helpless', 'hierarchy', 'in heaven',\n",
    "#  'inactive person', 'inconspicuous', 'land', 'lazy', 'lazy person', 'lost cause', 'low key', 'lowest point', 'near miss',\n",
    "#  'nervous wreck', 'old fashioned', 'old love', 'old lover', 'old news', 'old-fashioned', 'out of date', 'past love',\n",
    "#  'politically unstable', 'pushover', 'sedentary individual', 'shy', 'shy person', 'small nation', 'sweet', 'third world country',\n",
    "#  'top of the world', 'uninteresting', 'wallflower', 'wuss'\n",
    "# }\n",
    "\n",
    "# #\"keep a low profile\", \n",
    "# #\"pecking order\",\n",
    "# #\"old hat\",\n",
    "# #\"close call\",\n",
    "# #\"rock bottom\",\n",
    "# #\"basket case\",\n",
    "# #\"on cloud nine\",\n",
    "# #\"get in on the ground floor\",\n",
    "# #\"couch potato\",\n",
    "# #\"shrinking violet\",\n",
    "# #\"sitting duck\",\n",
    "# #\"an old flame\",\n",
    "# #\"banana republic\",\n",
    "\n",
    "\n",
    "# interesting_PIEs = {\n",
    "#     \"behind bars\", #in prison\n",
    "#     \"meet and talk in a friendly way\", #rub shoulders\n",
    "#     \"to awaken\", \"recall something\", #ring a bell\n",
    "#     \"take part in dangerous undertaking\", #play with fire\n",
    "#     \"to forgive someone\", #let bygones be bygones\n",
    "#     \"busy\", #chase your tail \n",
    "#     \"die\", \"passed away\", #kick the bucket\n",
    "#     \"disclose a secret\", \"reveal something\", #spill the beans\n",
    "#     \"spill the coffee\"\n",
    "# }\n",
    "\n",
    "# paraphrases_and_synonyms.update(nctti_synonyms)\n",
    "# paraphrases_and_synonyms.update(interesting_PIEs)\n",
    "# paraphrases_and_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65068a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e4bf0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idiom</th>\n",
       "      <th>idiom_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>act of God</td>\n",
       "      <td>IDactofgodID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>from scratch</td>\n",
       "      <td>IDfromscratchID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>chase your tail</td>\n",
       "      <td>IDchaseyourtailID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>under the table</td>\n",
       "      <td>IDunderthetableID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>at sea</td>\n",
       "      <td>IDatseaID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>game on</td>\n",
       "      <td>IDgameonID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>run a mile</td>\n",
       "      <td>IDrunamileID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>break the ice</td>\n",
       "      <td>IDbreaktheiceID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>in Dutch</td>\n",
       "      <td>IDindutchID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>lend a hand</td>\n",
       "      <td>IDlendahandID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>my bad</td>\n",
       "      <td>IDmybadID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>hold your tongue</td>\n",
       "      <td>IDholdyourtongueID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>daylight robbery</td>\n",
       "      <td>IDdaylightrobberyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>steal the show</td>\n",
       "      <td>IDstealtheshowID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>in the driver's seat</td>\n",
       "      <td>IDinthedriversseatID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>bite your lip</td>\n",
       "      <td>IDbiteyourlipID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>put the kibosh on</td>\n",
       "      <td>IDputthekiboshonID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     idiom           idiom_token\n",
       "33              act of God          IDactofgodID\n",
       "111           from scratch       IDfromscratchID\n",
       "114        chase your tail     IDchaseyourtailID\n",
       "131        under the table     IDunderthetableID\n",
       "153                 at sea             IDatseaID\n",
       "179                game on            IDgameonID\n",
       "362             run a mile          IDrunamileID\n",
       "500          break the ice       IDbreaktheiceID\n",
       "674               in Dutch           IDindutchID\n",
       "713            lend a hand         IDlendahandID\n",
       "813                 my bad             IDmybadID\n",
       "869       hold your tongue    IDholdyourtongueID\n",
       "870       daylight robbery   IDdaylightrobberyID\n",
       "1051        steal the show      IDstealtheshowID\n",
       "1093  in the driver's seat  IDinthedriversseatID\n",
       "1251         bite your lip       IDbiteyourlipID\n",
       "1373     put the kibosh on    IDputthekiboshonID"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the PIEs and token strings\n",
    "df_pie_token_mapping = pd.read_csv(token_PIE_mapping_file)\n",
    "\n",
    "# Consider only those PIEs that present in the WORDLIST_DICT\n",
    "df_pie_token_mapping = df_pie_token_mapping[df_pie_token_mapping['idiom_token'].isin(WORDLIST_DICT['PIE_list'])]\n",
    "\n",
    "if IS_BERTRAM_FORMAT:\n",
    "    # Convert the tokens to <BERTRAM:...> format\n",
    "    df_pie_token_mapping['idiom_token'] = df_pie_token_mapping['idiom_token'].map(lambda t: f\"<BERTRAM:{t}>\")\n",
    "    WORDLIST_DICT['PIE_list'] = [f\"<BERTRAM:{pie}>\" for pie in WORDLIST_DICT['PIE_list']]\n",
    "\n",
    "    print('Converted to BERTRAM format!')\n",
    "\n",
    "display(df_pie_token_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f01720",
   "metadata": {},
   "source": [
    "## Consider the words in the MAGPIE corpus\n",
    "\n",
    "**Consider only those sentences where the current list of PIEs are used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e448cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 1540 words from MAGPIE corpus\n"
     ]
    }
   ],
   "source": [
    "en_stopwords = stopwords.words('english')\n",
    "punc_remo_trans = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "en_stopwords.extend(WORDLIST_DICT['PIE_list'])\n",
    "en_stopwords = {s.lower() for s in en_stopwords}\n",
    "\n",
    "# Load the magpie training set\n",
    "MAGPIE_FULL_FILE = '/home/darshan/work/course/dissertation/idiom_principle_on_magpie_corpus/experiments/exp3A_1/tmp/magpie_full_exp3A_1.csv'\n",
    "df_magpie = pd.read_csv(MAGPIE_FULL_FILE)\n",
    "sent_list = df_magpie['sentence_0'].values\n",
    "\n",
    "MIN_COUNT = 1\n",
    "MAX_COUNT = 15\n",
    "MAGPIE_WORD_SET = set()\n",
    "# Add all the unique, non-stop words to a list (exclude the single tokens as well)\n",
    "word_counter = defaultdict(int)\n",
    "for sent in sent_list:\n",
    "    # Consider only those sentences that have the current list of PIEs\n",
    "    found=False\n",
    "    for pie in WORDLIST_DICT['PIE_list']:\n",
    "        if pie in sent:\n",
    "            found=True\n",
    "            break\n",
    "    if found:\n",
    "        sent = sent.translate(punc_remo_trans)\n",
    "        words = [word for word in sent.lower().split() if word not in en_stopwords]\n",
    "        words = [word for word in words if word.isalpha() and len(word)>2 and len(word)<=12]\n",
    "        # Count the word occurences\n",
    "        for word in words:\n",
    "            word_counter[word] += 1\n",
    "\n",
    "# Filter out very frequent and very rare words\n",
    "final_words = [word for word,count in word_counter.items() if count > MIN_COUNT and count < MAX_COUNT]\n",
    "MAGPIE_WORD_SET = set(final_words)\n",
    "\n",
    "print(f\"Obtained {len(MAGPIE_WORD_SET)} words from MAGPIE corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb6d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(sorted(word_counter.items(), key=lambda p: p[1]))\n",
    "len(MAGPIE_WORD_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d135b870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'simple', 'woman', 'spring', 'lived', 'radioactive', 'loss', 'bare', 'knots', 'whose', 'charles', 'diana', 'outfit', 'expenditure', 'meeting', 'luke', 'food', 'staff', 'vulcan', 'lines', 'happy', 'stopping', 'sizes', 'europe', 'enabling', 'betty', 'side', 'point', 'management', 'medal', 'committee', 'cost', 'expect', 'lexicons', 'somebody', 'drug', 'greater', 'french', 'offshore', 'study', 'always', 'king', 'northern', 'question', 'managers', 'door', 'unnecessary', 'storm', 'goods', 'easy', 'supply', 'sea', 'flood', 'tomorrow', 'waste', 'head', 'date', 'opened', 'ambitions', 'worse', 'scrapped', 'assembly', 'yes', 'need', 'stand', 'feels', 'apprentices', 'august', 'initially', 'friday', 'bus', 'house', 'displayed', 'members', 'finish', 'running', 'strokes', 'sons', 'internal', 'ground', 'spread', 'lowering', 'motto', 'act', 'ibm', 'tom', 'stops', 'investigate', 'memory', 'police', 'must', 'courts', 'duke', 'hero', 'warren', 'early', 'list', 'match', 'helping', 'commercial', 'scraps', 'ireland', 'body', 'fun', 'fifty', 'force', 'attack', 'meanwhile', 'lot', 'turner', 'captured', 'breeds', 'movement', 'kept', 'construct', 'robert', 'budget', 'states', 'damage', 'task', 'environment', 'distance', 'companion', 'arrived', 'particular', 'feeling', 'trainer', 'bottom', 'foreign', 'learned', 'shipping', 'protection', 'virtually', 'come', 'pressure', 'provides', 'remains', 'kick', 'pendulum', 'letters', 'bunny', 'jackson', 'chair', 'order', 'involved', 'tell', 'negligence', 'pepys', 'agreed', 'britain', 'therefore', 'exist', 'customers', 'group', 'fully', 'billiards', 'dumped', 'enemies', 'former', 'several', 'feet', 'history', 'direct', 'considering', 'set', 'facility', 'maritime', 'professional', 'throwbag', 'hit', 'bbc', 'health', 'winning', 'hope', 'gloucester', 'theyve', 'monday', 'fault', 'seen', 'move', 'fear', 'leaned', 'ask', 'development', 'north', 'tough', 'height', 'attractive', 'took', 'efforts', 'seems', 'firstly', 'later', 'football', 'maurice', 'flag', 'fashion', 'build', 'transvaal', 'september', 'fact', 'placed', 'publishing', 'based', 'flowering', 'encouraged', 'think', 'derby', 'attached', 'file', 'search', 'square', 'sort', 'suggest', 'responsible', 'result', 'property', 'person', 'mainly', 'across', 'community', 'reliable', 'human', 'games', 'challenge', 'power', 'expensive', 'unless', 'colleagues', 'package', 'manner', 'quality', 'stayed', 'learning', 'pilot', 'lifeboat', 'job', 'shook', 'league', 'engineering', 'wars', 'ensure', 'albania', 'davis', 'similar', 'odeon', 'lancashire', 'advantage', 'nick', 'sale', 'initiative', 'current', 'nature', 'architect', 'accepted', 'profusion', 'mighty', 'ruth', 'fleet', 'town', 'putting', 'boats', 'jason', 'helps', 'sent', 'held', 'gabriel', 'department', 'creating', 'called', 'collection', 'taligent', 'together', 'likes', 'burst', 'viable', 'silver', 'vehicle', 'create', 'kudos', 'rose', 'boy', 'national', 'primary', 'miles', 'really', 'game', 'fresh', 'full', 'died', 'joint', 'sense', 'dawn', 'ashore', 'strange', 'showing', 'card', 'living', 'court', 'main', 'night', 'days', 'cast', 'delighted', 'lord', 'blockade', 'capacity', 'limerick', 'upon', 'county', 'apparently', 'going', 'debate', 'cold', 'hills', 'lead', 'women', 'difficulty', 'highlights', 'works', 'hundred', 'cheaper', 'shoe', 'price', 'sure', 'leaders', 'throughout', 'rescue', 'ancient', 'obviously', 'jack', 'recycling', 'agricultural', 'division', 'far', 'harder', 'partly', 'binding', 'city', 'business', 'spoke', 'beach', 'knock', 'collective', 'men', 'class', 'son', 'dialects', 'experience', 'reduced', 'true', 'crew', 'sharpe', 'gazing', 'support', 'industrial', 'tea', 'provide', 'suddenly', 'disaster', 'connections', 'intelligence', 'though', 'edwards', 'lady', 'government', 'trouble', 'bank', 'trial', 'right', 'retrieve', 'blossom', 'sir', 'look', 'practice', 'certainly', 'office', 'telling', 'taking', 'england', 'bringing', 'frame', 'knees', 'weekend', 'competitors', 'sixty', 'lily', 'uncle', 'winter', 'planned', 'behind', 'parents', 'read', 'carrying', 'soon', 'decided', 'applies', 'scale', 'good', 'lay', 'known', 'significant', 'kid', 'land', 'sorting', 'missed', 'corp', 'enjoy', 'ban', 'world', 'clear', 'roads', 'elderly', 'stretching', 'wigan', 'popular', 'standard', 'moves', 'practical', 'sounds', 'eyes', 'brought', 'laugh', 'warm', 'drunk', 'effective', 'college', 'making', 'instance', 'fell', 'local', 'equipment', 'centre', 'handed', 'round', 'georgia', 'dogs', 'glass', 'meant', 'winds', 'materials', 'dictionaries', 'avoid', 'blank', 'standing', 'became', 'visitors', 'wind', 'mad', 'else', 'tickets', 'brownies', 'reach', 'rope', 'bad', 'told', 'various', 'answer', 'launching', 'certain', 'ice', 'miss', 'yet', 'voice', 'kingdom', 'unbelievably', 'assume', 'returns', 'extra', 'pressed', 'causes', 'interest', 'energy', 'china', 'rocks', 'wight', 'immediately', 'succession', 'gallery', 'merchant', 'mother', 'maybe', 'opportunity', 'less', 'finding', 'ahead', 'prime', 'jones', 'uncertain', 'stove', 'designer', 'officers', 'paragraph', 'continuing', 'paul', 'huge', 'trapped', 'eventually', 'coronet', 'sand', 'nuclear', 'rugby', 'street', 'strode', 'political', 'given', 'editor', 'policy', 'air', 'thinking', 'television', 'deep', 'arts', 'perfect', 'tyre', 'hands', 'sheila', 'swindon', 'something', 'track', 'agreement', 'entirely', 'steering', 'aviation', 'allowed', 'phoned', 'went', 'supporting', 'regularly', 'hurworth', 'aspect', 'helped', 'france', 'nothing', 'traditional', 'actually', 'want', 'edward', 'struggles', 'liberal', 'man', 'german', 'offering', 'using', 'course', 'drive', 'action', 'plans', 'friendly', 'west', 'lunch', 'settle', 'tonight', 'event', 'floor', 'final', 'field', 'taught', 'feel', 'problem', 'captain', 'slipped', 'press', 'original', 'gas', 'languages', 'increasingly', 'rebuilt', 'able', 'wish', 'plays', 'coming', 'nations', 'sun', 'straight', 'let', 'island', 'crawled', 'turn', 'raiders', 'party', 'sufficient', 'whole', 'contribute', 'bristol', 'today', 'semi', 'happened', 'end', 'finally', 'laptop', 'starts', 'leader', 'festival', 'stay', 'chest', 'york', 'white', 'countries', 'allies', 'chairman', 'poverty', 'battles', 'seem', 'crews', 'becoming', 'gather', 'ability', 'error', 'curton', 'dutch', 'ari', 'wanted', 'company', 'nearby', 'points', 'students', 'passed', 'swim', 'water', 'travelling', 'jumps', 'score', 'oclock', 'stare', 'megatape', 'authority', 'speaking', 'reform', 'moved', 'hour', 'art', 'offer', 'recordings', 'network', 'ship', 'wriggled', 'talking', 'begin', 'four', 'chosen', 'girl', 'faster', 'choose', 'chapel', 'understood', 'red', 'wednesday', 'fifteen', 'ready', 'allegedly', 'due', 'marine', 'losing', 'future', 'according', 'prestigious', 'daughter', 'democrats', 'fence', 'particularly', 'unlucky', 'asked', 'officer', 'groups', 'east', 'gone', 'alan', 'knowledge', 'parks', 'germany', 'fine', 'eton', 'thirty', 'design', 'content', 'say', 'attending', 'calls', 'often', 'afterwards', 'falling', 'deck', 'schools', 'investment', 'received', 'band', 'ever', 'procedure', 'saved', 'cup', 'keeping', 'mean', 'london', 'store', 'hat', 'desire', 'israel', 'portuguese', 'landing', 'launched', 'story', 'racing', 'excuse', 'aim', 'terry', 'pollution', 'nobody', 'giving', 'another', 'getting', 'train', 'install', 'father', 'pulled', 'latest', 'tasks', 'ups', 'leo', 'shaun', 'rabbits', 'open', 'designed', 'subject', 'scores', 'began', 'friend', 'earlier', 'low', 'ideas', 'characters', 'brave', 'eat', 'marks', 'sole', 'develop', 'cap', 'volatile', 'cooperation', 'bought', 'ones', 'cut', 'leading', 'tennis', 'eastern', 'process', 'occasion', 'opponent', 'outside', 'better', 'middle', 'safety', 'trained', 'radio', 'century', 'rest', 'whilst', 'coca', 'systems', 'saturdays', 'stop', 'drawing', 'ran', 'forms', 'victory', 'law', 'past', 'estimate', 'reached', 'beat', 'nicholas', 'jenkins', 'yeah', 'words', 'weeks', 'information', 'kind', 'areas', 'caught', 'impression', 'maker', 'success', 'bernard', 'none', 'next', 'needs', 'join', 'play', 'soviet', 'paid', 'give', 'zoffany', 'wood', 'imagine', 'road', 'supported', 'exclusive', 'williams', 'spanish', 'survival', 'june', 'drinking', 'doubt', 'compete', 'anything', 'described', 'bar', 'battle', 'launch', 'programme', 'ear', 'spent', 'smith', 'hauls', 'object', 'banned', 'exercise', 'ride', 'spain', 'done', 'video', 'names', 'thousand', 'complex', 'return', 'cover', 'everything', 'write', 'eye', 'villa', 'small', 'eighty', 'possible', 'margin', 'hundreds', 'developing', 'blue', 'carried', 'means', 'already', 'tracks', 'bed', 'relieved', 'methods', 'pitches', 'number', 'drowned', 'economic', 'morley', 'six', 'suppose', 'unix', 'effect', 'sky', 'whatever', 'van', 'operating', 'wardrobe', 'cat', 'credit', 'looks', 'healthy', 'moving', 'following', 'learn', 'attention', 'route', 'away', 'test', 'united', 'attended', 'stood', 'black', 'loan', 'seldom', 'museums', 'providing', 'easier', 'dead', 'shown', 'quick', 'repository', 'rigs', 'places', 'show', 'natural', 'association', 'green', 'clearly', 'fit', 'single', 'convention', 'ways', 'performance', 'inter', 'collision', 'watch', 'post', 'ducked', 'shock', 'choice', 'area', 'pitch', 'firms', 'guide', 'view', 'book', 'plant', 'perhaps', 'elsewhere', 'breast', 'curled', 'clash', 'fourth', 'goal', 'produced', 'education', 'defence', 'felt', 'harvest', 'example', 'experiences', 'security', 'comes', 'joining', 'production', 'live', 'voices', 'sides', 'reply', 'plaintiff', 'hear', 'gave', 'kicked', 'different', 'restricted', 'shall', 'pressures', 'seemed', 'impossible', 'understand', 'run', 'rather', 'share', 'knee', 'race', 'neil', 'might', 'federal', 'stock', 'jurisdiction', 'wrote', 'especially', 'conditions', 'fans', 'april', 'johnny', 'scottish', 'election', 'project', 'camera', 'variety', 'enough', 'friends', 'john', 'hot', 'touch', 'january', 'including', 'mrs', 'response', 'tested', 'much', 'failure', 'apart', 'quickly', 'earned', 'zelda', 'blockbuster', 'carry', 'wall', 'bit', 'soccer', 'church', 'experienced', 'met', 'tries', 'missing', 'chris', 'basis', 'break', 'rolled', 'turned', 'inc', 'site', 'peter', 'fisherman', 'midfield', 'asking', 'anyone', 'whale', 'crept', 'remove', 'saving', 'venter', 'large', 'peace', 'ordered', 'ultimately', 'laid', 'country', 'evil', 'corner', 'published', 'caused', 'luck', 'players', 'lives', 'leicester', 'series', 'form', 'hospital', 'growth', 'pity', 'please', 'nightmares', 'cross', 'minister', 'engine', 'president', 'rnli', 'fought', 'effort', 'per', 'previous', 'free', 'san', 'council', 'market', 'stuff', 'membership', 'powers', 'led', 'killed', 'case', 'quite', 'runners', 'heart', 'week', 'chat', 'looking', 'october', 'drink', 'endangered', 'prepared', 'hoping', 'ill', 'finished', 'taken', 'italy', 'border', 'absence', 'programs', 'things', 'nearly', 'catches', 'technology', 'explained', 'museum', 'yesterday', 'hours', 'looked', 'needed', 'strength', 'american', 'others', 'meet', 'missile', 'latin', 'software', 'normally', 'sitting', 'half', 'tyres', 'worked', 'research', 'torn', 'family', 'bon', 'got', 'space', 'windows', 'trade', 'discovered', 'heavy', 'bettie', 'shouted', 'leeds', 'along', 'working', 'team', 'knows', 'basically', 'option', 'front', 'foot', 'conversion', 'money', 'instead', 'inherited', 'star', 'river', 'corridor', 'painting', 'prevent', 'license', 'problems', 'follow', 'informed', 'brother', 'room', 'place', 'wooden', 'stage', 'texas', 'bird', 'congress', 'ball', 'computer', 'screens', 'courses', 'financial', 'wildlife', 'chain', 'philosophy', 'staying', 'ships', 'institutions', 'highly', 'pick', 'structure', 'forces', 'restaurant', 'royal', 'science', 'generally', 'insurance', 'recorded', 'eccentric', 'stars', 'lower', 'despite', 'attempt', 'village', 'save', 'sound', 'came', 'export', 'moment', 'chipped', 'structures', 'dinky', 'seeing', 'boundaries', 'sail', 'november', 'enemy', 'driven', 'robinson', 'busy', 'found', 'grand', 'near', 'receiving', 'eldest', 'services', 'selling', 'beyond', 'attempting', 'appear', 'sentences', 'someone', 'mill', 'literary', 'wore', 'windmills', 'goes', 'almost', 'hide', 'indeed', 'catch', 'compared', 'installed', 'wheel', 'garden', 'pads', 'fwiday', 'important', 'picked', 'broad', 'part', 'building', 'complicated', 'latter', 'ministry', 'firm', 'eight', 'present', 'win', 'whether', 'fire', 'pre', 'park', 'knew', 'scored', 'keep', 'saw', 'sit', 'considered', 'negotiations', 'miraculous', 'violent', 'towards', 'husband', 'companies', 'readers', 'age', 'somewhere', 'thought', 'nevertheless', 'general', 'winger', 'fight', 'jenny', 'find', 'union', 'regulations', 'personal', 'tried', 'children', 'language', 'suggested', 'club', 'dog', 'vehicles', 'born', 'month', 'talk', 'religious', 'italian', 'terms', 'necessary', 'efficient', 'remember', 'conscious', 'university', 'breaks', 'among', 'issue', 'naval', 'organization', 'expects', 'expected', 'completely', 'position', 'publicity', 'willing', 'gracious', 'singing', 'stroke', 'overheads', 'vessels', 'cope', 'safe', 'started', 'cent', 'death', 'necessarily', 'major', 'bohinen', 'speak', 'leaving', 'shake', 'usually', 'stuck', 'cash', 'fish', 'shoes', 'researchers', 'plot', 'months', 'ago', 'champion', 'rule', 'gon', 'tuesday', 'neck', 'plate', 'yards', 'brown', 'consumed', 'heard', 'control', 'happen', 'successfully', 'fast', 'says', 'line', 'successful', 'pattern', 'sharing', 'bedroom', 'growing', 'buildings', 'report', 'system', 'top', 'eager', 'served', 'products', 'youngster', 'minutes', 'richmond', 'steps', 'excellent', 'traffic', 'fellow', 'plane', 'statutory', 'consequences', 'finale', 'difference', 'broken', 'display', 'texts', 'entwined', 'opposition', 'longer', 'oil', 'riders', 'interested', 'manager', 'collapse', 'thing', 'culture', 'help', 'school', 'planning', 'qualified', 'likely', 'player', 'high', 'five', 'operation', 'modern', 'required', 'hand', 'however', 'mention', 'tells', 'english', 'introduced', 'try', 'possibly', 'everyone', 'junk', 'hill', 'probably', 'writing', 'rough', 'cups', 'demand', 'acquired', 'cruise', 'wickets', 'lost', 'entire', 'fairly', 'home', 'gossip', 'shop', 'luxury', 'british', 'allocate', 'boat', 'poison', 'amateur', 'change', 'times', 'length', 'visit', 'machine', 'saying', 'printed', 'played', 'equivalent', 'training', 'appointment', 'walk', 'seven', 'common', 'playing', 'grabbing', 'legs', 'mostly', 'sat', 'wrist', 'grounds', 'technical', 'collect', 'special', 'lear', 'matches', 'thanks', 'morning', 'risk', 'dumping', 'continued', 'crucial', 'level', 'convinced', 'young', 'changes', 'secondly', 'existing', 'truly', 'sister', 'face', 'window', 'occur', 'seat', 'hms', 'formed', 'goals', 'spurs', 'lightning', 'refused', 'opposed', 'charlotte', 'name', 'least', 'big', 'available', 'army', 'kicking', 'established', 'lamp', 'magazine', 'third', 'cricket', 'designated', 'younger', 'erm', 'billy', 'boys', 'tour', 'channel', 'compensation', 'fed', 'factor', 'written', 'storms', 'media', 'release', 'hard', 'size', 'recaptured', 'season', 'bid', 'fingers', 'colchester', 'prop', 'buried', 'idea', 'station', 'within', 'fall', 'kitchen', 'disk', 'profile', 'province', 'created', 'complete', 'evening', 'bob', 'rival', 'escape', 'snooker', 'covered', 'theft', 'james', 'model', 'film', 'confident', 'state', 'newton', 'pulling', 'ulster', 'stockport', 'close', 'headmaster', 'european', 'decision', 'biggest', 'coast', 'haunted', 'sight', 'western', 'failed', 'physical', 'tae', 'impressed', 'scotland', 'belfast', 'tide', 'guidelines', 'memories', 'monitoring', 'thus', 'wild', 'kansas', 'direction', 'box', 'occupied', 'note', 'never', 'trying', 'serious', 'record', 'either', 'okay', 'remembered', 'seeking', 'defences', 'central', 'mezzanine', 'couple', 'category', 'behaviour', 'users', 'continue'}\n"
     ]
    }
   ],
   "source": [
    "print(MAGPIE_WORD_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f0518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca653da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory check\n",
    "if os.path.isdir(dump_dir):\n",
    "    raise Exception(f\"Output directory {dump_dir} already exists!\")\n",
    "else:\n",
    "    os.makedirs(dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d252deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../local_models/bert-base-uncased_MaskedLM_STR_option1_3B1_pretrained were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../local_models/bert-base-uncased_MaskedLM_STR_option1_3B1_pretrained and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded both the LM Model & the Tokenizer models\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model & tokenizers\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_dir)\n",
    "# Download the Tokenizer model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_dir, use_fast=True, truncation=True)\n",
    "print(f\"Loaded both the LM Model & the Tokenizer models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26d163f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32260, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding matrix\n",
    "embedding_weights = model.bert.embeddings.word_embeddings.weight\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f804f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15f261f8",
   "metadata": {},
   "source": [
    "# Create tokens-id mapping for all the tokens\n",
    "We need to get the embeddings for all the tokens and the constituent words of the PIEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964fd234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got token ids for 17 PIE single tokens and 38 words\n"
     ]
    }
   ],
   "source": [
    "#To store PIE singel tokens\n",
    "id_to_single_token_mapping = {}\n",
    "# To store constituent words of a PIE\n",
    "word_to_ids = {}\n",
    "\n",
    "for i,(pie, token_str) in df_pie_token_mapping.iterrows():\n",
    "    # First, get the id-token mapping for 'token_str'\n",
    "    token_id = tokenizer.vocab[token_str.lower()]\n",
    "    # Add to the dict\n",
    "    id_to_single_token_mapping[token_id] = token_str\n",
    "    \n",
    "    # Next, process the individual words in the pie, find their token ids\n",
    "    pie_words = [pword.strip() for pword in pie.split() if pword not in word_to_ids]\n",
    "    for pword in pie_words:\n",
    "        token_ids = tokenizer.encode(pword, add_special_tokens=False)\n",
    "        word_to_ids[pword] = token_ids\n",
    "        \n",
    "print(f\"Got token ids for {len(id_to_single_token_mapping)} PIE single tokens and {len(word_to_ids)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6537da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max possible length of a line containing tab separated embeddings, a rough estimate\n",
    "MAX_EMB_LINE_LENGTH = (64 + 1)*768\n",
    "\n",
    "def get_single_token_embeddings(tok_id):\n",
    "    emb_vec = embedding_weights[tok_id].detach().numpy()\n",
    "    return emb_vec\n",
    "\n",
    "def get_embedding_string(emb_vec):\n",
    "    \"\"\"\n",
    "    # Convert one embedding vector from numpy array into a tab separated string format\n",
    "    # NOTE: The float64 precision is used here!!\n",
    "    \"\"\"\n",
    "    emb_str = np.array2string(emb_vec, separator='\\t', \\\n",
    "                              max_line_width=MAX_EMB_LINE_LENGTH, \\\n",
    "                              formatter={'float_kind':lambda x: str(np.float64(x))}, \\\n",
    "                              suppress_small=False, floatmode='maxprec')\n",
    "    # Trim [ and ] characters\n",
    "    emb_str = emb_str[1:-1]\n",
    "    return emb_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f89f66",
   "metadata": {},
   "source": [
    "## Word to Embedding string mapping\n",
    "Create a map of <tok_word, embedding_str> for both pie single-tokens and the constituent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f951f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created word-embedding string mapping for 55 tokens\n"
     ]
    }
   ],
   "source": [
    "word_emb_str_mapping = {}\n",
    "\n",
    "# First process all the pie single-tokens\n",
    "for tok_id, tok_word in id_to_single_token_mapping.items():\n",
    "    emb_vec = get_single_token_embeddings(tok_id)\n",
    "    emb_str = get_embedding_string(emb_vec)\n",
    "    word_emb_str_mapping[tok_word] = emb_str\n",
    "    \n",
    "# Then process all the constituent words(Note: we have array of subtokens per each word!)\n",
    "def get_average_embedding(token_ids):\n",
    "    all_emb_vecs = []\n",
    "    for tok_id in token_ids:\n",
    "        emb_vec = get_single_token_embeddings(tok_id)\n",
    "        all_emb_vecs.append(emb_vec)\n",
    "    np_embs = np.array(all_emb_vecs)\n",
    "    avg_emb = np_embs.mean(axis=0)\n",
    "    return avg_emb\n",
    "    \n",
    "for word, tok_ids in word_to_ids.items():\n",
    "    emb_vec = get_average_embedding(tok_ids)\n",
    "    emb_str = get_embedding_string(emb_vec)\n",
    "    word_emb_str_mapping[word] = emb_str\n",
    "    \n",
    "print(f\"Created word-embedding string mapping for {len(word_emb_str_mapping)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2a737",
   "metadata": {},
   "source": [
    "### Add sample paraphrases and synonyms\n",
    "Add the paraphrases and synonyms as well. The embeddings are obtained by averaging the embeddings of the subtokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66912f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added additional paraphrases & synonyms of sample PIEs.\n",
      "The final word-embedding string mapping contains 86 tokens\n"
     ]
    }
   ],
   "source": [
    "COMMON_PARAPHRASES = WORDLIST_DICT['paraphrases']\n",
    "for paraphrase in COMMON_PARAPHRASES:\n",
    "    # Get the average embeddings for each paraphrase\n",
    "    tok_ids = tokenizer.encode(paraphrase, add_special_tokens=False)\n",
    "    emb_vec = get_average_embedding(tok_ids)\n",
    "    para_emb_str = get_embedding_string(emb_vec)\n",
    "\n",
    "    # Append to the mapping dictionary\n",
    "    word_emb_str_mapping[paraphrase] = para_emb_str\n",
    "    \n",
    "print(\"Added additional paraphrases & synonyms of sample PIEs.\")\n",
    "print(f\"The final word-embedding string mapping contains {len(word_emb_str_mapping)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94fa70",
   "metadata": {},
   "source": [
    "### Add the MAGPIE words as well\n",
    "Add all the words selected from MAGPIE corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f706bb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added words from MAGPIE corpus!\n",
      "The final word-embedding string mapping contains 1614 tokens\n"
     ]
    }
   ],
   "source": [
    "for mword in MAGPIE_WORD_SET:\n",
    "    # Get the average embeddings for each word\n",
    "    tok_ids = tokenizer.encode(mword, add_special_tokens=False)\n",
    "    emb_vec = get_average_embedding(tok_ids)\n",
    "    mword_emb_str = get_embedding_string(emb_vec)\n",
    "\n",
    "    # Append to the mapping dictionary\n",
    "    word_emb_str_mapping[mword] = mword_emb_str\n",
    "    \n",
    "print(\"Added words from MAGPIE corpus!\")\n",
    "print(f\"The final word-embedding string mapping contains {len(word_emb_str_mapping)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f96078",
   "metadata": {},
   "source": [
    "## Save the tokens and embedding strings into separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4118cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of words:1586\n"
     ]
    }
   ],
   "source": [
    "# Maintain a very specific order of words and be consistent across different experiments!!!\n",
    "# 1. Combine the PIEs, paraphrases, the words from WORDLIST_DICT and the MAGPIE words \n",
    "# 2. Remove duplicates!!\n",
    "# 3. and sort them!!\n",
    "final_word_set = set(WORDLIST_DICT['PIE_list'] + WORDLIST_DICT['paraphrases'] + WORDLIST_DICT['words'] + list(MAGPIE_WORD_SET))\n",
    "SORTED_word_list = list(sorted(final_word_set))\n",
    "\n",
    "print(f'Final number of words:{len(SORTED_word_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4506ddb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved word and embedding TSV files at ./embedding_dump/\n"
     ]
    }
   ],
   "source": [
    "outfile_prefix = exp_name + '_' + wordlist_set_NAME\n",
    "word_file_path = os.path.join(dump_dir, outfile_prefix + '_words.tsv')\n",
    "embedding_file_path = os.path.join(dump_dir, outfile_prefix + '_vectors.tsv')\n",
    "\n",
    "# For every token in the additional tokens, get the embedding vector\n",
    "# Append the token to word_file, append the tab-separated emebeddings to embeddings_file\n",
    "with open(word_file_path, 'a') as word_file:\n",
    "    with open(embedding_file_path, 'a') as embedding_file:\n",
    "        # Write to the file in the exact same order!!\n",
    "        for tok_word in SORTED_word_list:\n",
    "            emb_str = word_emb_str_mapping[tok_word]\n",
    "            # Save tok_word\n",
    "            word_file.write(tok_word + '\\n')\n",
    "            # Save emb_str\n",
    "            embedding_file.write(emb_str + '\\n')\n",
    "\n",
    "print(f'Saved word and embedding TSV files at {dump_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016170e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB_VENV",
   "language": "python",
   "name": "lab_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
