{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad63ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:../../local_models/bert-base-uncased_SequenceClassification_STR_option1\n"
     ]
    }
   ],
   "source": [
    "local_model_base_dir = '../../local_models/'\n",
    "# Location of the model with single-tokens\n",
    "model_name = 'bert-base-uncased_SequenceClassification_STR_option1'\n",
    "model_checkpoint_dir = local_model_base_dir + model_name\n",
    "print(f'Model path:{model_checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049b9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment name\n",
    "exp_name = 'exp3A_1'\n",
    "\n",
    "IS_BERTRAM_FORMAT = False\n",
    "\n",
    "token_PIE_mapping_file = '../../data/token_files/option1_idioms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c970d93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should BERTRAM format be used: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Should BERTRAM format be used: {IS_BERTRAM_FORMAT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906d7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "dump_dir = './embedding_dump/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc62cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79aef008",
   "metadata": {},
   "source": [
    "# Add paraphrases of sample PIEs\n",
    "In addition to the PIE single tokens and their constituent words, synonyms and paraphrases for some of the interesting PIEs are also included in the embedding space for the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520e0ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absolute lowest',\n",
       " 'all time low',\n",
       " 'basement',\n",
       " 'behind bars',\n",
       " 'bliss',\n",
       " 'bottom floor',\n",
       " 'busy',\n",
       " 'cheapest',\n",
       " 'cliffhanger',\n",
       " 'close one',\n",
       " 'close shave',\n",
       " 'crazy',\n",
       " 'dated',\n",
       " 'defenseless',\n",
       " 'die',\n",
       " 'disclose a secret',\n",
       " 'down low',\n",
       " 'easy prey',\n",
       " 'easy target',\n",
       " 'euphoria',\n",
       " 'ex girlfriend',\n",
       " 'exposed',\n",
       " 'first floor',\n",
       " 'first story',\n",
       " 'food chain',\n",
       " 'former lover',\n",
       " 'ground level',\n",
       " 'heaven',\n",
       " 'helpless',\n",
       " 'hierarchy',\n",
       " 'in heaven',\n",
       " 'inactive person',\n",
       " 'inconspicuous',\n",
       " 'land',\n",
       " 'lazy',\n",
       " 'lazy person',\n",
       " 'lost cause',\n",
       " 'low key',\n",
       " 'lowest point',\n",
       " 'meet and talk in a friendly way',\n",
       " 'near miss',\n",
       " 'nervous wreck',\n",
       " 'old fashioned',\n",
       " 'old love',\n",
       " 'old lover',\n",
       " 'old news',\n",
       " 'old-fashioned',\n",
       " 'out of date',\n",
       " 'passed away',\n",
       " 'past love',\n",
       " 'politically unstable',\n",
       " 'pushover',\n",
       " 'recall something',\n",
       " 'reveal something',\n",
       " 'sedentary individual',\n",
       " 'shy',\n",
       " 'shy person',\n",
       " 'small nation',\n",
       " 'spill the coffee',\n",
       " 'sweet',\n",
       " 'take part in dangerous undertaking',\n",
       " 'third world country',\n",
       " 'to awaken',\n",
       " 'to forgive someone',\n",
       " 'top of the world',\n",
       " 'uninteresting',\n",
       " 'wallflower',\n",
       " 'wuss'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases_and_synonyms = set()\n",
    "\n",
    "nctti_synonyms = {\n",
    " 'absolute lowest', 'all time low', 'basement', 'bliss', 'bottom floor', 'cheapest', 'cliffhanger', 'close one',\n",
    " 'close shave', 'crazy', 'dated', 'defenseless', 'down low', 'easy prey', 'easy target', 'euphoria', 'ex girlfriend', 'exposed',\n",
    " 'first floor', 'first story', 'food chain', 'former lover', 'ground level', 'heaven', 'helpless', 'hierarchy', 'in heaven',\n",
    " 'inactive person', 'inconspicuous', 'land', 'lazy', 'lazy person', 'lost cause', 'low key', 'lowest point', 'near miss',\n",
    " 'nervous wreck', 'old fashioned', 'old love', 'old lover', 'old news', 'old-fashioned', 'out of date', 'past love',\n",
    " 'politically unstable', 'pushover', 'sedentary individual', 'shy', 'shy person', 'small nation', 'sweet', 'third world country',\n",
    " 'top of the world', 'uninteresting', 'wallflower', 'wuss'\n",
    "}\n",
    "\n",
    "#\"keep a low profile\", \n",
    "#\"pecking order\",\n",
    "#\"old hat\",\n",
    "#\"close call\",\n",
    "#\"rock bottom\",\n",
    "#\"basket case\",\n",
    "#\"on cloud nine\",\n",
    "#\"get in on the ground floor\",\n",
    "#\"couch potato\",\n",
    "#\"shrinking violet\",\n",
    "#\"sitting duck\",\n",
    "#\"an old flame\",\n",
    "#\"banana republic\",\n",
    "\n",
    "\n",
    "interesting_PIEs = {\n",
    "    \"behind bars\", #in prison\n",
    "    \"meet and talk in a friendly way\", #rub shoulders\n",
    "    \"to awaken\", \"recall something\", #ring a bell\n",
    "    \"take part in dangerous undertaking\", #play with fire\n",
    "    \"to forgive someone\", #let bygones be bygones\n",
    "    \"busy\", #chase your tail \n",
    "    \"die\", \"passed away\", #kick the bucket\n",
    "    \"disclose a secret\", \"reveal something\", #spill the beans\n",
    "    \"spill the coffee\"\n",
    "}\n",
    "\n",
    "paraphrases_and_synonyms.update(nctti_synonyms)\n",
    "paraphrases_and_synonyms.update(interesting_PIEs)\n",
    "paraphrases_and_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b072d00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251a9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e4bf0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idiom</th>\n",
       "      <th>idiom_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>off the beaten track</td>\n",
       "      <td>IDoffthebeatentrackID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the running</td>\n",
       "      <td>IDintherunningID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>give someone the creeps</td>\n",
       "      <td>IDgivesomeonethecreepsID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do someone proud</td>\n",
       "      <td>IDdosomeoneproudID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take root</td>\n",
       "      <td>IDtakerootID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     idiom               idiom_token\n",
       "0     off the beaten track     IDoffthebeatentrackID\n",
       "1           in the running          IDintherunningID\n",
       "2  give someone the creeps  IDgivesomeonethecreepsID\n",
       "3         do someone proud        IDdosomeoneproudID\n",
       "4                take root              IDtakerootID"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the PIEs and token strings\n",
    "df_pie_token_mapping = pd.read_csv(token_PIE_mapping_file)\n",
    "if IS_BERTRAM_FORMAT:\n",
    "    # Convert the tokens to <BERTRAM:...> format\n",
    "    df_pie_token_mapping['idiom_token'] = df_pie_token_mapping['idiom_token'].map(lambda t: f\"<BERTRAM:{t}>\")\n",
    "    print('Converted to BERTRAM format')\n",
    "\n",
    "df_pie_token_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca653da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory check\n",
    "if os.path.isdir(dump_dir):\n",
    "    raise Exception(f\"Output directory {dump_dir} already exists!\")\n",
    "else:\n",
    "    os.makedirs(dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d252deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded both the LM Model & the Tokenizer models\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model & tokenizers\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_dir)\n",
    "# Download the Tokenizer model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_dir, use_fast=True, truncation=True)\n",
    "print(f\"Loaded both the LM Model & the Tokenizer models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26d163f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32260, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding matrix\n",
    "embedding_weights = model.bert.embeddings.word_embeddings.weight\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f804f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15f261f8",
   "metadata": {},
   "source": [
    "# Create tokens-id mapping for all the tokens\n",
    "We need to get the embeddings for all the tokens and the constituent words of the PIEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "964fd234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got token ids for 1738 PIE single tokens and 1735 words\n"
     ]
    }
   ],
   "source": [
    "#To store PIE singel tokens\n",
    "id_to_single_token_mapping = {}\n",
    "# To store constituent words of a PIE\n",
    "word_to_ids = {}\n",
    "\n",
    "for i,(pie, token_str) in df_pie_token_mapping.iterrows():\n",
    "    # First, get the id-token mapping for 'token_str'\n",
    "    token_id = tokenizer.vocab[token_str.lower()]\n",
    "    # Add to the dict\n",
    "    id_to_single_token_mapping[token_id] = token_str\n",
    "    \n",
    "    # Next, process the individual words in the pie, find their token ids\n",
    "    pie_words = [pword.strip() for pword in pie.split() if pword not in word_to_ids]\n",
    "    for pword in pie_words:\n",
    "        token_ids = tokenizer.encode(pword, add_special_tokens=False)\n",
    "        word_to_ids[pword] = token_ids\n",
    "        \n",
    "print(f\"Got token ids for {len(id_to_single_token_mapping)} PIE single tokens and {len(word_to_ids)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6537da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max possible length of a line containing tab separated embeddings, a rough estimate\n",
    "MAX_EMB_LINE_LENGTH = (64 + 1)*768\n",
    "\n",
    "def get_single_token_embeddings(tok_id):\n",
    "    emb_vec = embedding_weights[tok_id].detach().numpy()\n",
    "    return emb_vec\n",
    "\n",
    "def get_embedding_string(emb_vec):\n",
    "    \"\"\"\n",
    "    # Convert one embedding vector from numpy array into a tab separated string format\n",
    "    # NOTE: The float64 precision is used here!!\n",
    "    \"\"\"\n",
    "    emb_str = np.array2string(emb_vec, separator='\\t', \\\n",
    "                              max_line_width=MAX_EMB_LINE_LENGTH, \\\n",
    "                              formatter={'float_kind':lambda x: str(np.float64(x))}, \\\n",
    "                              suppress_small=False, floatmode='maxprec')\n",
    "    # Trim [ and ] characters\n",
    "    emb_str = emb_str[1:-1]\n",
    "    return emb_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f89f66",
   "metadata": {},
   "source": [
    "## Word to Embedding string mapping\n",
    "Create a map of <tok_word, embedding_str> for both pie single-tokens and the constituent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f951f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created word-embedding string mapping for 3473 tokens\n"
     ]
    }
   ],
   "source": [
    "word_emb_str_mapping = {}\n",
    "\n",
    "# First process all the pie single-tokens\n",
    "for tok_id, tok_word in id_to_single_token_mapping.items():\n",
    "    emb_vec = get_single_token_embeddings(tok_id)\n",
    "    emb_str = get_embedding_string(emb_vec)\n",
    "    word_emb_str_mapping[tok_word] = emb_str\n",
    "    \n",
    "# Then process all the constituent words(Note: we have array of subtokens per each word!)\n",
    "def get_average_embedding(token_ids):\n",
    "    all_emb_vecs = []\n",
    "    for tok_id in token_ids:\n",
    "        emb_vec = get_single_token_embeddings(tok_id)\n",
    "        all_emb_vecs.append(emb_vec)\n",
    "    np_embs = np.array(all_emb_vecs)\n",
    "    avg_emb = np_embs.mean(axis=0)\n",
    "    return avg_emb\n",
    "    \n",
    "for word, tok_ids in word_to_ids.items():\n",
    "    emb_vec = get_average_embedding(tok_ids)\n",
    "    emb_str = get_embedding_string(emb_vec)\n",
    "    word_emb_str_mapping[word] = emb_str\n",
    "    \n",
    "print(f\"Created word-embedding string mapping for {len(word_emb_str_mapping)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be475512",
   "metadata": {},
   "source": [
    "### Add sample paraphrases and synonyms\n",
    "Add the paraphrases and synonyms as well. The embeddings are obtained by averaging the embeddings of the subtokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba6dbb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added additional paraphrases & synonyms of sample PIEs.\n",
      "The final word-embedding string mapping contains 3536 tokens\n"
     ]
    }
   ],
   "source": [
    "for paraphrase in paraphrases_and_synonyms:\n",
    "    # Get the average embeddings for each paraphrase\n",
    "    tok_ids = tokenizer.encode(paraphrase, add_special_tokens=False)\n",
    "    emb_vec = get_average_embedding(tok_ids)\n",
    "    para_emb_str = get_embedding_string(emb_vec)\n",
    "\n",
    "    # Append to the mapping dictionary\n",
    "    word_emb_str_mapping[paraphrase] = para_emb_str\n",
    "    \n",
    "print(\"Added additional paraphrases & synonyms of sample PIEs.\")\n",
    "print(f\"The final word-embedding string mapping contains {len(word_emb_str_mapping)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f96078",
   "metadata": {},
   "source": [
    "## Save the tokens and embedding strings into separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c75bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4506ddb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved word and embedding TSV files at ./embedding_dump/\n"
     ]
    }
   ],
   "source": [
    "word_file_path = os.path.join(dump_dir, exp_name+'_words.tsv')\n",
    "embedding_file_path = os.path.join(dump_dir, exp_name+'_vectors.tsv')\n",
    "\n",
    "# For every token in the additional tokens, get the embedding vector\n",
    "# Append the token to word_file, append the tab-separated emebeddings to embeddings_file\n",
    "with open(word_file_path, 'a') as word_file:\n",
    "    with open(embedding_file_path, 'a') as embedding_file:\n",
    "        for tok_word, emb_str in word_emb_str_mapping.items():\n",
    "            # Save tok_word\n",
    "            word_file.write(tok_word + '\\n')\n",
    "            # Save emb_str\n",
    "            embedding_file.write(emb_str + '\\n')\n",
    "\n",
    "print(f'Saved word and embedding TSV files at {dump_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB_VENV",
   "language": "python",
   "name": "lab_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
